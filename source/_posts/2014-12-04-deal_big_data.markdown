---
title: "一次Bash处理数据的经历"
layout: post
category: bash
tagline: ""
---



### 背景

Nginx访问日志中记录了客户端的访问信息，根据访问URL中带的参数，可以进行行为分析，  
当前日志中记录了过去7天的访问日志，每天的访问日志根据时间分别存放在24个不同的文件中，  
一天的24个日志文件的总大小约20G左右，为了追踪用户手机ID，及该用户的手机机型、  
操作系统型号、APP版本号，需从日志中提取相关的信息


### 过程


**1.Shell单进程提取**

>	选取符合条件的数据文件，逐个处理文件，
>
>	逐行读取文件，判断改行是否符合条件，
>
>	如果符合条件，则从改行选取有用的字段；
>
>	对该字段进行后续的字符串操作，两级split处理
>
>	多个文件的处理结果追加到结果文件中，
>
>	追加的时候进行查重处理，重复的数据不追加
>

*	**处理半小时后，未结束，生成结果速度明显减慢,5个小时未出最终结果**
*	**怀疑原始大文件读取到一定行数述速度减慢**
*	**怀疑追加结果前查重处理当结果数增加时影响查重速度**
*	**故实现多进程版本，希望加速处理**

**2.Shell多进程提取**

>	选取符合条件的数据文件，
>
>	每个数据文件建立子Shell进行处理，
>
>	处理结果写到独立的结果文件中，追加前进行查重处理，
>
>	将多个子Shell的结果进行合并除重
>

*	**处理半小时后，未结束，生成结果速度明显减慢,3个小时未出最终结果**
*	**怀疑追加结果前查重处理当结果数增加时影响查重速度**
*	**故实现Python版本，处理结果常驻内存增加处理速度**

**3.Python数据常驻内存处理**

>
>	选取其中一个数据文件，
>
>	逐行读取文件，逐行处理，
>
>	处理结果存放到内存中，在内存中除重，
>
>	生成所有结果后，写入到文件中，
>


*	**处理半小时后，未结束，3个小时未出最终结果**
*	**怀疑Python脚本语言本身速度的问题**
*	**实现C++版本**

**4.C++处理**

>	处理过程同Python实现，
>
>	只是将实现语言改为C++
>


**5.Shell预处理，数据提取**

>	选取符合条件的数据文件，24个，
>
>	每个文件用一个子Shell处理，
>
>	每个子进程首先对该进程负责的文件进行预处理，
>
>	从该文件着中提取符合条件的行，生成新的小文件，
>
>	子Shell继续逐行处理该小文件，对子文件的每一行进行字符串处理，从中获取键值对，
>
>	生成该文件的处理结果，追加到文件中，
>
>	子Shell继续处理新生成的文件，进行排序，去重处理，生成该文件的最终结果文件
>
>	对24个结果文件，进行合并，排序，除重处理
>

*	**实现C++的版本过程中，觉着未必是语言的问题**
*	**仔细分析后，觉着可能是大数据文件频繁多次读取的问题**
*	**故对原始文件进行预处理，生成临时中间小文件，进行后续复杂处理**
*	**实验证明预处理后，明显的提高了处理速度**


### 总结


1.	操作大文件时，最好一次操作完毕，进行数据预处理或者消除冗余数据，生成小数据文件

2.	尽量不要频繁的操作文件，将数据放在内存中处理


