<!DOCTYPE html> <html> <head> <meta charset="utf-8"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title> sqoop使用入门 | Moses </title> <meta name="description" content=" Fucking Big Data "> <meta name="keywords" content="Hadoop, Spark, Storm, Kafka, Akaka, BigData"> <meta name="HandheldFriendly" content="True"> <meta name="MobileOptimized" content="320"> <!-- Social: Facebook / Open Graph --> <meta property="og:type" content="article"> <meta property="article:author" content="Moses"> <meta property="article:section" content="others"> <meta property="article:tag" content=""> <meta property="article:published_time" content="2015-01-20 00:00:00 +0800"> <meta property="og:url" content="http://lifulong.me/2015/sqoop/"> <meta property="og:title" content=" sqoop使用入门 | Moses "> <meta property="og:image" content="http://lifulong.me"> <meta property="og:description" content=" Fucking Big Data "> <meta property="og:site_name" content="Moses"> <meta property="og:locale" content="pt_BR"> <!-- Social: Twitter --> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:site" content="@leefulong"> <meta name="twitter:title" content=" sqoop使用入门 | Moses "> <meta name="twitter:description" content=" Fucking Big Data "> <meta name="twitter:image:src" content="http://lifulong.me"> <!-- Social: Google+ / Schema.org --> <meta itemprop="name" content=" sqoop使用入门 | Moses "> <meta itemprop="description" content=" Fucking Big Data "> <meta itemprop="image" content="http://lifulong.me"> <!-- rel prev and next --> <link rel="stylesheet" href="/assets/css/main.css"> <!-- Canonical link tag --> <link rel="canonical" href="http://lifulong.me/2015/sqoop/"> <link rel="alternate" type="application/rss+xml" title="Moses" href="http://lifulong.me/feed.xml"> <script type="text/javascript"> var disqus_shortname = 'lifulong'; var _gaq = _gaq || []; _gaq.push(['_setAccount', 'UA-52446115-1']); _gaq.push(['_trackPageview']); (function() { var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true; ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s); })(); </script> </head> <body> <main class="wrapper"> <header class="site-header"> <nav class="nav"> <div class="container"> <h1 class="logo"><a href="/">Moses<span>blog</span></a></h1> <ul class="navbar"> <li><a href="/about">about</a></li> <li><a href="/feed.xml" target="_blank">feed</a></li> </ul> </div> </nav> </header> <article class="post container" itemscope itemtype="http://schema.org/BlogPosting"> <header class="post-header"> <h1 class="post-title" itemprop="name headline">sqoop使用入门</h1> <p class="post-meta"><time datetime="2015-01-20T00:00:00+08:00" itemprop="datePublished">Jan 20, 2015</time></p> </header> <div class="post-content" itemprop="articleBody"> <h2 id="sqoop">sqoop安装</h2> <p>1.<strong>安装hadoop2.6.0(2.x)</strong></p> <p>到hadoop官网，下载最新的hadoop，<a href="http://mirror.cc.columbia.edu/pub/software/apache/hadoop/common/">下载链接 http://mirror.cc.columbia.edu/pub/software/apache/hadoop/common/</a></p> <blockquote><pre><code>wget http://mirror.cc.columbia.edu/pub/software/apache/hadoop/common/stable/hadoop-2.6.0.tar.gz
</code></pre></blockquote> <p>或</p> <blockquote><pre><code>axel http://mirror.cc.columbia.edu/pub/software/apache/hadoop/common/stable/hadoop-2.6.0.tar.gz
</code></pre></blockquote> <p>创建hadoop的根目录，压缩包解压缩后，将解压缩的目录拷贝到hadoop根目录中，作为hadoop根目录的子目录<br /> 参考<a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">官网</a>，对hadoop进行配置</p> <p>ps:</p> <p>hadoop可以对namenode,datanode,tmpdir,logdir进行配置</p> <p>修改etc/hadoop/hadoop/core-site.xml</p> <blockquote><pre><code>&lt;property&gt;
    &lt;name&gt;dfs.name.dir&lt;/name&gt;
	&lt;!--
		//NameNode持久存储名字空间及事务日志的本地文件系统路径。
	--&gt;
	&lt;value&gt;/data/hdfs/hdfs1&lt;/value&gt;
&lt;/property&gt;
 
&lt;property&gt;
	&lt;!--
	//DataNode存放块数据的本地文件系统路径，逗号分割的列表。
	--&gt;
	&lt;name&gt;dfs.data.dir&lt;/name&gt;
	&lt;value&gt;/data/hdfs/hdfsdata1&lt;/value&gt;
&lt;/property&gt;
 
&lt;property&gt;
	&lt;!--
	//Hadoop的默认临时路径，这个最好配置，然后在新增节点或者其他情况下莫名其妙的DataNode启动不了，
	//就删除此文件中的tmp目录即可。不过如果删除了NameNode机器的此目录，那么就需要重新执行NameNode格式化的命令了。
	--&gt;
	&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
	&lt;value&gt;/data/hdfs/hdfstmp&lt;/value&gt;
	&lt;description&gt;A base for other temporary directories.&lt;/description&gt;
&lt;/property&gt;
</code></pre></blockquote> <p>修改etc/hadoop/hadoop-env.sh文件</p> <blockquote><pre><code>export HADOOP_LOG_DIR=/data/hadooplogs/
</code></pre></blockquote> <p>2.<strong>安装hive</strong></p> <blockquote><pre><code>wget http://apache.dataguru.cn/hive/stable/apache-hive-0.14.0-bin.tar.gz
</code></pre></blockquote> <p>配置:</p> <ul> <li>cp conf/hive-default.xml.template conf/hive-default.xml</li> <li>cp conf/hive-env.sh.template conf/hive-env.sh</li> </ul> <p>touch hive-site.xml</p> <blockquote><pre><code>&lt;configuration&gt;

	&lt;property&gt;
		&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
		&lt;value&gt;jdbc:mysql://localhost:3306/hive?createData baseIfNotExist=true&lt;/value&gt;
		&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;
	&lt;/property&gt;

	&lt;property&gt;
		&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
		&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
		&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;
	&lt;/property&gt;

	&lt;property&gt;
		&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
		&lt;value&gt;root&lt;/value&gt;
		&lt;description&gt;username to use against metastore database&lt;/description&gt;
	&lt;/property&gt;

	&lt;property&gt;
		&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
		&lt;value&gt;test&lt;/value&gt;
		&lt;description&gt;password to use against metastore database&lt;/description&gt;
	&lt;/property&gt;

&lt;/configuration&gt;
</code></pre></blockquote> <p>将解压缩的目录拷贝到hadoop的根目录中，作为一个子目录，进行配置文件的配置</p> <p>3.<strong>安装hbase</strong></p> <blockquote><pre><code>wget http://apache.fayea.com/hbase/stable/hbase-0.98.9-hadoop2-bin.tar.gz
</code></pre></blockquote> <p>4.<strong>sqoop安装</strong></p> <p>下载支持hadoop2.x的sqoop可执行压缩包</p> <blockquote><pre><code>wget http://psg.mtu.edu/pub/apache/sqoop/1.4.5/sqoop-1.4.5.bin__hadoop-2.0.4-alpha.tar.gz	
</code></pre></blockquote> <p>将解压缩的目录拷贝到hadoop根目录中，进行配置文件的配置</p> <p>下载java mysql connector,并拷贝到sqoop/lib/目录,<a href="http://dev.mysql.com/downloads/connector/j/5.0.html">connector下载地址</a>.</p> <h2 id="sqoop-1">sqoop命令</h2> <p>1.<strong>codegen:将数据库表生成jar文件</strong></p> <blockquote><pre><code>sqoop/bin/sqoop codegen --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table book
</code></pre></blockquote> <p><em>该文件可通过mapreduce执行???未验证</em></p> <p>2.<strong>eval:快速验证sql语句的执行结果</strong></p> <blockquote><pre><code>sqoop/bin/sqoop eval --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root -query "SELECT * FROM book LIMIT 10"
</code></pre></blockquote> <p><em>执行结果显示在控制台</em></p> <p>3.<strong>查询数据库列表</strong></p> <blockquote><pre><code>sqoop/bin/sqoop list-databases --connect jdbc:mysql://localhost:3306/ -username root -password root
</code></pre></blockquote> <p>4.<strong>查询数据库中的所有表</strong></p> <blockquote><pre><code>sqoop/bin/sqoop list-tables --connect jdbc:mysql://localhost:3306/test_sqoop -username root -password root
</code></pre></blockquote> <p>5.<strong>导入数据到hdfs</strong></p> <blockquote><pre><code>sqoop/bin/sqoop import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table book -m 1 --target-dir /user/hive/result
</code></pre></blockquote> <ul> <li>参数说明</li> </ul> <blockquote><pre><code>--append
--warehouse-dir &lt;dir&gt;:与--target-dir不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录
</code></pre></blockquote> <p>6.<strong>合并hdfs中数据</strong></p> <p>7.<strong>复制表结构</strong></p> <blockquote><pre><code>sqoop/bin/sqoop create-hive-table --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table book
</code></pre></blockquote> <p>8.<strong>导入mysql表中数据到hive</strong></p> <blockquote><pre><code>sqoop/bin/sqoop import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table book --fields-terminated-by "\t" --lines-terminated-by "\n" -m 1 --hive-import --hive-overwrite --create-hive-table --hive-table book --delete-target-dir
</code></pre></blockquote> <ul> <li>参数说明</li> </ul> <blockquote><pre><code>--table &lt;table-name&gt;:关系数据库表名，数据从该表中获取

--boundary-query &lt;statement&gt;:查询的字段中不能有数据类型为字符串的字段

--columns&lt;col1,col2,col…&gt;:

--split-by&lt;column-name&gt;:表的列名，用来切分工作单元，一般后面跟主键ID

--query，-e&lt;statement&gt;:	从查询结果中导入数据，该参数使用时必须指定--target-dir、--hive-table，在查询语句中一定要有where条件且在where条件中需要包含$CONDITION

--where statement:从关系数据库导入数据时的查询条件，--where "id = 2"

--target-dir &lt;dir&gt;:指定hdfs路径

--null-string &lt;null-string&gt;: string类型的字段值为null时的填充值

--null-non-string &lt;null-string&gt;: 非string类型的字段值为null时的填充值

-m,--num-mappers n:启动N个map来并行导入数据，默认是4个

--direct:直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快
</code></pre></blockquote> <p>9.<strong>增量导入mysql数据到hive</strong></p> <blockquote><pre><code>sqoop/bin/sqoop import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table book --fields-terminated-by "\t" --lines-terminated-by "\n" -m 1  --check-column id --incremental append --last-value 4 --hive-import --hive-table book

--check-column col:用来作为判断的列名，如id
--incremental mode:
&gt;	append：追加，比如对大于last-value指定的值之后的记录进行追加导入
&gt;	lastmodified：最后的修改时间，追加last-value指定的日期之后的记录

--last-value value:指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值
</code></pre></blockquote> <p>10.<strong>导入数据库中的所有表到hdfs</strong></p> <blockquote><pre><code>sqoop/bin/sqoop import-all-tables --connect jdbc:mysql://localhost:3306/test_sqoop
</code></pre></blockquote> <p>11.<strong>导入数据库中的所有表到hive</strong></p> <blockquote><pre><code>sqoop/bin/sqoop import-all-tables --connect jdbc:mysql://localhost:3306/test_sqoop	--hive-import
</code></pre></blockquote> <p>12.<strong>生成job</strong></p> <blockquote><pre><code>sqoop/bin/sqoop job --create myjob  --import --connectjdbc:mysql://localhost:3306/test_sqoop --table book
sqoop/bin/sqoop job --exec myjob
</code></pre></blockquote> <p>13.<strong>free form query import</strong></p> <p>Sqoop支持导入查询结果集. Instead of using the –table, –columns and –where arguments, you can specify a SQL statement with the –query argument.</p> <p>通过–query方式导入时必须指定导入目标地址 –target-dir.(<em>通过hdfs文件的方式操作hive</em>)</p> <p>If you want to import the results of a query in parallel, then each map task will need to execute a copy of the query, with results partitioned by bounding conditions inferred by Sqoop. Your query must include the token $CONDITIONS which each Sqoop process will replace with a unique condition expression. You must also select a splitting column with –split-by.</p> <blockquote><pre><code>sqoop import --query 'SELECT * FROM book WHERE id=3 AND $CONDITIONS' --split-by id --target-dir /user/hive/book
</code></pre></blockquote> <h2 id="mysqlhive">增量更新mysql到hive策略</h2> <p>1.<strong>借助job及时间戳</strong></p> <p>需数据库中存在时间戳字段(timestamp类型)</p> <blockquote><pre><code>sqoop/bin/sqoop job --create incretest -- import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table incretest -m 1 --hive-import --hive-overwrite --hive-table INCRETEST --append --incremental lastmodified --check-column update_time --last-value '2015/1/20 10:00:00'
</code></pre></blockquote> <blockquote><pre><code>sqoop/bin/sqoop job --create incretest -- import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table incretest -m 1 --hive-import --hive-table INCRETEST --append --incremental lastmodified --check-column update_time --last-value '2015/1/20 10:00:00' 多次执行次job，sqoop job会自动将起始时间更新为job上次执行的时间，已验证
</code></pre></blockquote> <blockquote><pre><code>sqoop/bin/sqoop job --exec incretest
</code></pre></blockquote> <p>2.<strong>借助job及 increase id</strong></p> <p>需数据库中存在increase id(int类型)</p> <blockquote><pre><code>sqoop/bin/sqoop job --create import_book -- import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root --table book  --fields-terminated-by "\t" --lines-terminated-by "\n" -m 1 --hive-import --hive-table book --incremental append --check-column id --last-value '0'
</code></pre></blockquote> <p>多次执行次job，sqoop job会自动将起始ID更新为job上次执行的Upper_ID，已验证</p> <blockquote><pre><code>sqoop/bin/sqoop job --exec incretest
</code></pre></blockquote> <p>3.<strong>借助hive工具</strong></p> <blockquote><pre><code>$SQOOP_HOME/bin/sqoop import --connect ${rdbms_connstr} --username ${rdbms_username} --password ${rdbms_pwd} --table ${rdbms_table} --columns "${rdbms_columns}" --where "CREATE_TIME &gt; to_date('${startdate}','yyyy-mm-dd hh24:mi:ss') and CREATE_TIME &lt; to_date('${enddate}','yyyy-mm-dd hh24:mi:ss')" --hive-import --hive-overwrite --hive-table ${hive_increment_table}
</code></pre></blockquote> <blockquote><pre><code>$HIVE_HOME/bin/hive -e "insert overwrite table ${hive_full_table} select * from ${hive_increment_table} union all select a.* from ${hive_full_table} a left outer join ${hive_increment_table} b on a.service_code = b.service_code where b.service_code is null;"
</code></pre></blockquote> <p>4.<strong>文件方式操作hive</strong></p> <blockquote><pre><code>sqoop/bin/sqoop import --connect jdbc:mysql://localhost:3306/test_sqoop --username root --password root  -e "select * from book where id=4 and \$CONDITIONS"  --fields-terminated-by "\t" --lines-terminated-by "\n" --append --as-textfile --target-dir /user/hive/warehouse/book -m 1
</code></pre></blockquote> <h2 id="bug">BUG</h2> <p>–hive-overwrite 与 –hive-partition-key –hive-partition-value 不能同时使用 create-hive-table 命令中 –hive-table 与 –hive-partition-key 不能同时使用</p> <h2 id="ref">Ref</h2> <ol> <li><a href="http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html">官方文档http://sqoop.apache.org/docs/1.4.4/SqoopUserGuide.html</a></li> <li><a href="http://www.zihou.me/html/2014/01/28/9114.html/comment-page-1">sqoop命令详解</a></li> </ol> <aside class="share"> <h4>Share this.</h4> <a href="http://twitter.com/share?text=sqoop使用入门&amp;url=http://lifulong.me/2015/sqoop/&amp;hashtags=web,dev,blog,soudev&amp;via=nandomoreirame" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">Twitter</a> <a href="https://www.facebook.com/sharer/sharer.php?u=http://lifulong.me/2015/sqoop/" onclick="window.open(this.href, 'facebook-share', 'width=550,height=235');return false;">Facebook</a> </aside> </div> </article> <footer class="site-footer"> <div class="container"> <small class="pull-left">&copy;2018 All rights reserved. Made with <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> and ♥</small> <small class="pull-right">by <a href="http://lifulong.github.io/" target="_blank">lifulong.me</a></small> </div> </footer> </main> </body> </html>
